{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional_neural_networks_solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mancinimassimiliano/DeepLearningLab/blob/master/Lab2/solution/convolutional_neural_networks_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kaht-FPA1Jvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Lab2: Train a Convolutional Neural Network (CNN).\n",
        "\n",
        "In this Lab session we will learn how to train a CNN from scratch for classifying MNIST digits."
      ]
    },
    {
      "metadata": {
        "id": "UvxtTYHlVfRK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HYCvhGxKWyN7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define LeNet\n",
        "\n",
        "![alt text](https://1569708099.rsc.cdn77.org/wp-content/uploads/2017/09/lenet-5.png)\n",
        "\n",
        "Here we are going to define our first CNN which is **LeNet** in this case. To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n",
        "\n",
        "Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26)."
      ]
    },
    {
      "metadata": {
        "id": "dMC_LDYdWkI7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    \n",
        "    # input channel = 1, output channels = 6, kernel size = 5\n",
        "    # input image size = (28, 28), image output size = (24, 24)\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5, 5))\n",
        "    \n",
        "    # input channel = 6, output channels = 16, kernel size = 5\n",
        "    # input image size = (12, 12), output image size = (8, 8)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5))\n",
        "    \n",
        "    # input dim = 4 * 4 * 16 ( H x W x C), output dim = 120\n",
        "    self.fc3 = torch.nn.Linear(in_features=4 * 4 * 16, out_features=120)\n",
        "    \n",
        "    # input dim = 120, output dim = 84\n",
        "    self.fc4 = torch.nn.Linear(in_features=120, out_features=84)\n",
        "    \n",
        "    # input dim = 84, output dim = 10\n",
        "    self.fc5 = torch.nn.Linear(in_features=84, out_features=10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    # Max Pooling with kernel size = 2\n",
        "    # output size = (12, 12)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    \n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    # Max Pooling with kernel size = 2\n",
        "    # output size = (4, 4)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    \n",
        "    # flatten the feature maps into a long vector\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    \n",
        "    x = self.fc3(x)\n",
        "    x = F.relu(x)\n",
        "    \n",
        "    x = self.fc4(x)\n",
        "    x = F.relu(x)\n",
        "    \n",
        "    x = self.fc5(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gChf6TvWonrV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define cost function"
      ]
    },
    {
      "metadata": {
        "id": "6j5UrBH3oek8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2TjXeVdorV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the optimizer"
      ]
    },
    {
      "metadata": {
        "id": "hBZN-WPboulR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTkfrV64oxIL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "metadata": {
        "id": "t-sE5vFio0lf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6IT0Lsgo8AM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training.\n",
        "\n",
        "We will learn a new thing in this function as how to Normalize the inputs given to the network.\n",
        "\n",
        "***Why Normalization is needed***? \n",
        "\n",
        "To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\]. \n",
        "\n",
        "***How it can be done***? \n",
        "\n",
        "This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize)."
      ]
    },
    {
      "metadata": {
        "id": "qDxpo6uVo_8k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "  \n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform = list()\n",
        "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\n",
        "  transform = T.Compose(transform)                          # Composes the above transformations into one.\n",
        "\n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True) \n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \n",
        "  \n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OHcB8f0AsY4n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "ip_R-hruse0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.01, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=25):\n",
        "  \n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "  \n",
        "  net = LeNet().to(device)\n",
        "  \n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltdCMiB3t18h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets train!"
      ]
    },
    {
      "metadata": {
        "id": "6d-z20H4tziL",
        "colab_type": "code",
        "outputId": "d0590fdc-52b5-4612-f5d1-c26bbece67cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2699
        }
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before training:\n",
            "\t Training loss 0.01806, Training accuracy 12.11\n",
            "\t Validation loss 0.00907, Validation accuracy 12.00\n",
            "\t Test loss 0.00922, Test accuracy 12.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.01062, Training accuracy 57.08\n",
            "\t Validation loss 0.00103, Validation accuracy 91.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00133, Training accuracy 94.71\n",
            "\t Validation loss 0.00047, Validation accuracy 96.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00080, Training accuracy 96.80\n",
            "\t Validation loss 0.00037, Validation accuracy 97.05\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00060, Training accuracy 97.58\n",
            "\t Validation loss 0.00028, Validation accuracy 97.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00049, Training accuracy 98.10\n",
            "\t Validation loss 0.00028, Validation accuracy 97.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00040, Training accuracy 98.29\n",
            "\t Validation loss 0.00023, Validation accuracy 98.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00034, Training accuracy 98.64\n",
            "\t Validation loss 0.00025, Validation accuracy 98.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00031, Training accuracy 98.81\n",
            "\t Validation loss 0.00023, Validation accuracy 98.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00028, Training accuracy 98.91\n",
            "\t Validation loss 0.00021, Validation accuracy 98.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00023, Training accuracy 99.16\n",
            "\t Validation loss 0.00021, Validation accuracy 98.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00023, Training accuracy 99.06\n",
            "\t Validation loss 0.00022, Validation accuracy 98.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00018, Training accuracy 99.28\n",
            "\t Validation loss 0.00021, Validation accuracy 98.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00018, Training accuracy 99.31\n",
            "\t Validation loss 0.00021, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00015, Training accuracy 99.40\n",
            "\t Validation loss 0.00021, Validation accuracy 98.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00013, Training accuracy 99.44\n",
            "\t Validation loss 0.00020, Validation accuracy 98.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00012, Training accuracy 99.50\n",
            "\t Validation loss 0.00021, Validation accuracy 98.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00010, Training accuracy 99.60\n",
            "\t Validation loss 0.00023, Validation accuracy 98.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00009, Training accuracy 99.63\n",
            "\t Validation loss 0.00026, Validation accuracy 98.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00008, Training accuracy 99.65\n",
            "\t Validation loss 0.00026, Validation accuracy 98.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00006, Training accuracy 99.78\n",
            "\t Validation loss 0.00021, Validation accuracy 98.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00005, Training accuracy 99.82\n",
            "\t Validation loss 0.00023, Validation accuracy 98.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.00005, Training accuracy 99.82\n",
            "\t Validation loss 0.00025, Validation accuracy 98.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.00005, Training accuracy 99.84\n",
            "\t Validation loss 0.00024, Validation accuracy 98.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.00004, Training accuracy 99.85\n",
            "\t Validation loss 0.00024, Validation accuracy 98.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.00003, Training accuracy 99.86\n",
            "\t Validation loss 0.00025, Validation accuracy 98.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.00003, Training accuracy 99.90\n",
            "\t Validation loss 0.00025, Validation accuracy 98.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.00004, Training accuracy 99.84\n",
            "\t Validation loss 0.00025, Validation accuracy 98.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.00003, Training accuracy 99.91\n",
            "\t Validation loss 0.00025, Validation accuracy 98.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.00001, Training accuracy 99.97\n",
            "\t Validation loss 0.00026, Validation accuracy 98.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.00001, Training accuracy 99.97\n",
            "\t Validation loss 0.00025, Validation accuracy 98.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.00001, Training accuracy 99.99\n",
            "\t Validation loss 0.00025, Validation accuracy 98.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.00000, Training accuracy 99.99\n",
            "\t Validation loss 0.00026, Validation accuracy 98.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00026, Validation accuracy 98.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 34\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00026, Validation accuracy 98.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 35\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00027, Validation accuracy 98.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 36\n",
            "\t Training loss 0.00000, Training accuracy 100.00\n",
            "\t Validation loss 0.00027, Validation accuracy 98.79\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}